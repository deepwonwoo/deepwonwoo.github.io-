---
title: [TIL] EM 알고리즘
layout: post
date: "2021-01-03 12:00:00"
author: deepwonwoo
tags: TIL EM algorithm
cover: "/assets/keayboard1.jpg"
categories: TIL
---



## EM(Expectation Maximization) 알고리즘

많은 종류의 기계 학습 모델은 likelihood를 최대화하는 방식으로 학습이 진행된다. 직접 관측할 수 없는 latent variable이 있는 경우 이는 다음과 같은 식으로 결정된다.
$$
\arg\max_{\mathbf{\theta}} \log p(\mathbf{X}|\mathbf{\theta}) = \arg\max_{\mathbf{\theta}} \log \int_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z}|\mathbf{\theta}) d\mathbf{Z}
$$


위의 식의 최적화는 gradient descent 기법으로도 가능하지만 θθ에 제약조건이 있을 때에는 이를 문제 없이 처리하기 위한 노력이 필요하다. 가령 Gaussian Mixture Model (GMM)을 생각해보면, 클러스터에 속할 확률은 모두 더해서 1을 만족해야 하고, 각 가우스 분포의 공분산은 positive semi-definite 해야 하는 조건을 만족해야 하는데 단순하게 gardient descent를 수행하면 이런 제약 조건을 깨는 경우가 생길 수 있다.

반면 gradient descent 와는 다르게 latent variable model 에서의 학습을 다루는 방법으로 expectation maximization (EM) 기법이 있다. 대체로 EM은 위에서 설명한 문제가 없고 이후에 [variational inference](https://shurain.net/personal-perspective/variational-inference)로 이어지기에 적합한 접근 방법이다.



다음은 EM알고리즘을 간략히 요약한 것이다.

1. 매개변수 ![\boldsymbol{\theta}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf)를 임의의 값으로 설정한다.
2. 주어진 매개변수 값에 관한 잠재변수 ![{\mathbf  {Z}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b776aaf12c2da4b78ca777cb8295c2000bfd51f5)값을 추정한다.
3. 2단계에서 얻은 잠재변수 값을 이용해 매개변수 ![\boldsymbol{\theta}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf)값을 다시 추정한다.
4. 매개변수 ![\boldsymbol{\theta}](https://wikimedia.org/api/rest_v1/media/math/render/svg/33b025a6bf54ec02e65c871dc3e5897c921419cf)값과 잠재변수 ![{\mathbf  {Z}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b776aaf12c2da4b78ca777cb8295c2000bfd51f5)값이 수렴할 때까지 2, 3단계를 반복한다.

